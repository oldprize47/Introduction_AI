{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b89df7",
   "metadata": {},
   "source": [
    "## In-class Activity 1: Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61214a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is nlp???  \n",
      "Natural Language Processing, or NLP for short, is broadly defined as the automatic manipulation of natural language, like speech and text, by software.\n",
      "The study of natural language processing has been around for more than 50 years and grew out of the field of linguistics with the rise of computers.\n",
      "(In this post), you will discover what natural language processing is and why it is so important.\n",
      "After reading this post, you will know => What natural language is and how it is different from other types of data.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "def remove_html(text_data):\n",
    "    soup = BeautifulSoup(text_data, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "str_data = \"<html><h2>What is nlp??? </h2></html> \\nNatural Language Processing, or NLP for short, is broadly defined as the automatic manipulation of natural language, like speech and text, by software.\\nThe study of natural language processing has been around for more than 50 years and grew out of the field of linguistics with the rise of computers.\\n(In this post), you will discover what natural language processing is and why it is so important.\\nAfter reading this post, you will know => What natural language is and how it is different from other types of data.\"\n",
    "processed_text = remove_html(str_data)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7ad6f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print('Punctuation: ', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21fcaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    sent = []\n",
    "    for t in text.split(' '):\n",
    "        no_punct = ''.join([c for c in t if c not in string.punctuation])\n",
    "        sent.append(no_punct)\n",
    "\n",
    "    sentence = \" \".join([s for s in sent])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5c3829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is nlp  \n",
      "Natural Language Processing or NLP for short is broadly defined as the automatic manipulation of natural language like speech and text by software\n",
      "The study of natural language processing has been around for more than 50 years and grew out of the field of linguistics with the rise of computers\n",
      "In this post you will discover what natural language processing is and why it is so important\n",
      "After reading this post you will know  What natural language is and how it is different from other types of data\n"
     ]
    }
   ],
   "source": [
    "rmv_punc_sentence = remove_punctuation(processed_text)\n",
    "print(rmv_punc_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b558e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is nlp  \n",
      "natural language processing or nlp for short is broadly defined as the automatic manipulation of natural language like speech and text by software\n",
      "the study of natural language processing has been around for more than 50 years and grew out of the field of linguistics with the rise of computers\n",
      "in this post you will discover what natural language processing is and why it is so important\n",
      "after reading this post you will know  what natural language is and how it is different from other types of data\n"
     ]
    }
   ],
   "source": [
    "lower_sentence = rmv_punc_sentence.lower()\n",
    "print(lower_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61f88b",
   "metadata": {},
   "source": [
    "## In-class Activity 2: Tokenization & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6dab8a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9e6f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d930f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(lower_sentence.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f6f5bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('what', 'what'), ('is', 'be'), ('nlp', 'nlp'), (' \\n', ' \\n'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('or', 'or'), ('nlp', 'nlp'), ('for', 'for'), ('short', 'short'), ('is', 'be'), ('broadly', 'broadly'), ('defined', 'define'), ('as', 'as')]\n"
     ]
    }
   ],
   "source": [
    "tok_lem_sentence = [(token.text, token.lemma_) for token in doc]\n",
    "print(tok_lem_sentence[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743eed34",
   "metadata": {},
   "source": [
    "## In-class Activity 3: Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "860c5200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sangheon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec29f3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english')[:10])\n",
    "print(len(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d396699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stopword removal:  ['what', 'be', 'nlp', ' \\n', 'natural', 'language', 'processing', 'or', 'nlp', 'for', 'short', 'be', 'broadly', 'define', 'as', 'the', 'automatic', 'manipulation', 'of', 'natural', 'language', 'like', 'speech', 'and', 'text', 'by', 'software', '\\n', 'the', 'study', 'of', 'natural', 'language', 'processing', 'have', 'be', 'around', 'for', 'more', 'than', '50', 'year', 'and', 'grow', 'out', 'of', 'the', 'field', 'of', 'linguistic', 'with', 'the', 'rise', 'of', 'computer', '\\n', 'in', 'this', 'post', 'you', 'will', 'discover', 'what', 'natural', 'language', 'processing', 'be', 'and', 'why', 'it', 'be', 'so', 'important', '\\n', 'after', 'read', 'this', 'post', 'you', 'will', 'know', ' ', 'what', 'natural', 'language', 'be', 'and', 'how', 'it', 'be', 'different', 'from', 'other', 'type', 'of', 'datum']\n",
      "\n",
      "After stopword removal:  ['nlp', ' \\n', 'natural', 'language', 'processing', 'nlp', 'short', 'broadly', 'define', 'automatic', 'manipulation', 'natural', 'language', 'like', 'speech', 'text', 'software', '\\n', 'study', 'natural', 'language', 'processing', 'around', '50', 'year', 'grow', 'field', 'linguistic', 'rise', 'computer', '\\n', 'post', 'discover', 'natural', 'language', 'processing', 'important', '\\n', 'read', 'post', 'know', ' ', 'natural', 'language', 'different', 'type', 'datum']\n",
      "\n",
      "Removed words:  ['what', 'be', 'or', 'for', 'be', 'as', 'the', 'of', 'and', 'by', 'the', 'of', 'have', 'be', 'for', 'more', 'than', 'and', 'out', 'of', 'the', 'of', 'with', 'the', 'of', 'in', 'this', 'you', 'will', 'what', 'be', 'and', 'why', 'it', 'be', 'so', 'after', 'this', 'you', 'will', 'what', 'be', 'and', 'how', 'it', 'be', 'from', 'other', 'of']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lem_sentence = [w[1] for w in tok_lem_sentence]\n",
    "print(\"Before stopword removal: \", lem_sentence)\n",
    "\n",
    "rmv_sw_sentence = [w for w in lem_sentence if w not in stop_words]\n",
    "print(\"\\nAfter stopword removal: \", rmv_sw_sentence)\n",
    "\n",
    "romoved_word = [w for w in lem_sentence if w not in rmv_sw_sentence]\n",
    "print(\"\\nRemoved words: \", romoved_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b42527",
   "metadata": {},
   "source": [
    "## In-class Activity 4: Encoding Text as Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa858302",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "\n",
    "def make_frequency_dict(text):\n",
    "    for word in text:\n",
    "        if word not in dictionary:\n",
    "            dictionary[word] = 0\n",
    "        dictionary[word] += 1\n",
    "\n",
    "make_frequency_dict(rmv_sw_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c868a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11cfc4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nlp': 2,\n",
       " ' \\n': 1,\n",
       " 'natural': 5,\n",
       " 'language': 5,\n",
       " 'processing': 3,\n",
       " 'short': 1,\n",
       " 'broadly': 1,\n",
       " 'define': 1,\n",
       " 'automatic': 1,\n",
       " 'manipulation': 1,\n",
       " 'like': 1,\n",
       " 'speech': 1,\n",
       " 'text': 1,\n",
       " 'software': 1,\n",
       " '\\n': 3,\n",
       " 'study': 1,\n",
       " 'around': 1,\n",
       " '50': 1,\n",
       " 'year': 1,\n",
       " 'grow': 1,\n",
       " 'field': 1,\n",
       " 'linguistic': 1,\n",
       " 'rise': 1,\n",
       " 'computer': 1,\n",
       " 'post': 2,\n",
       " 'discover': 1,\n",
       " 'important': 1,\n",
       " 'read': 1,\n",
       " 'know': 1,\n",
       " ' ': 1,\n",
       " 'different': 1,\n",
       " 'type': 1,\n",
       " 'datum': 1}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "55eaf07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('natural', 5),\n",
       " ('language', 5),\n",
       " ('processing', 3),\n",
       " ('\\n', 3),\n",
       " ('nlp', 2),\n",
       " ('post', 2),\n",
       " (' \\n', 1),\n",
       " ('short', 1),\n",
       " ('broadly', 1),\n",
       " ('define', 1),\n",
       " ('automatic', 1),\n",
       " ('manipulation', 1),\n",
       " ('like', 1),\n",
       " ('speech', 1),\n",
       " ('text', 1),\n",
       " ('software', 1),\n",
       " ('study', 1),\n",
       " ('around', 1),\n",
       " ('50', 1),\n",
       " ('year', 1),\n",
       " ('grow', 1),\n",
       " ('field', 1),\n",
       " ('linguistic', 1),\n",
       " ('rise', 1),\n",
       " ('computer', 1),\n",
       " ('discover', 1),\n",
       " ('important', 1),\n",
       " ('read', 1),\n",
       " ('know', 1),\n",
       " (' ', 1),\n",
       " ('different', 1),\n",
       " ('type', 1),\n",
       " ('datum', 1)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sorted = sorted(dictionary.items(), key = lambda x: x[1], reverse=True)\n",
    "vocab_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29c2cf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural': 1, 'language': 2, 'processing': 3, '\\n': 4, 'nlp': 5, 'post': 6}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "\n",
    "for (word, frequency) in vocab_sorted:\n",
    "    if frequency > 1:\n",
    "        i += 1\n",
    "        word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b623e003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural': 1, 'language': 2, 'processing': 3, '\\n': 4, 'nlp': 5, 'post': 6, '<OOV>': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_index['<OOV>'] = len(word_to_index) + 1\n",
    "print(word_to_index\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cfa44838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed words:  ['nlp', ' \\n', 'natural', 'language', 'processing', 'nlp', 'short', 'broadly', 'define', 'automatic', 'manipulation', 'natural', 'language', 'like', 'speech', 'text', 'software', '\\n', 'study', 'natural', 'language', 'processing', 'around', '50', 'year', 'grow', 'field', 'linguistic', 'rise', 'computer', '\\n', 'post', 'discover', 'natural', 'language', 'processing', 'important', '\\n', 'read', 'post', 'know', ' ', 'natural', 'language', 'different', 'type', 'datum']\n",
      "\n",
      "Encoded words:  [5, 7, 1, 2, 3, 5, 7, 7, 7, 7, 7, 1, 2, 7, 7, 7, 7, 4, 7, 1, 2, 3, 7, 7, 7, 7, 7, 7, 7, 7, 4, 6, 7, 1, 2, 3, 7, 4, 7, 6, 7, 7, 1, 2, 7, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "encoded = []\n",
    "\n",
    "print(\"Processed words: \", rmv_sw_sentence)\n",
    "\n",
    "for w in rmv_sw_sentence:\n",
    "    try:\n",
    "        encoded.append(word_to_index[w])\n",
    "    except KeyError:\n",
    "        encoded.append(word_to_index['<OOV>'])\n",
    "\n",
    "print(\"\\nEncoded words: \", encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27722d6d",
   "metadata": {},
   "source": [
    "## In-class Activity 5: One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "228bcc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 words and one hot vectors:\n",
      "\n",
      "1. word: 'nlp'\n",
      "    One-hot: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "2. word: ' \n",
      "'\n",
      "    One-hot: [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "3. word: 'natural'\n",
      "    One-hot: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "4. word: 'language'\n",
      "    One-hot: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "5. word: 'processing'\n",
      "    One-hot: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab = sorted(set(rmv_sw_sentence))\n",
    "vocab_size = len(vocab)\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "one_hot_matrix = np.zeros((len(rmv_sw_sentence), vocab_size), dtype = int)\n",
    "for i, word in enumerate(rmv_sw_sentence):\n",
    "    index = word_to_index[word]\n",
    "    one_hot_matrix[i, index] = 1\n",
    "\n",
    "print(\"First 5 words and one hot vectors:\\n\")\n",
    "for i in range(5):\n",
    "    word = rmv_sw_sentence[i]\n",
    "    vector = one_hot_matrix[i]\n",
    "    print(f\"{i+1}. word: '{word}'\")\n",
    "    print(f\"    One-hot: {vector}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
